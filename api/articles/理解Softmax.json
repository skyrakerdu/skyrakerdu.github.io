{"title":"理解Softmax","uid":"edfb80c4b09d73ff93916de362feb42e","slug":"理解Softmax","date":"2022-03-11T03:21:58.000Z","updated":"2022-03-11T05:03:38.526Z","comments":true,"path":"api/articles/理解Softmax.json","keywords":null,"cover":[],"content":"<h3 id=\"1-Softmax\"><a href=\"#1-Softmax\" class=\"headerlink\" title=\"1.Softmax\"></a>1.Softmax</h3><p>Softmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。</p>\n<p>hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。往往在实际中这种方式是不合情理的，比如对于文本分类来说，一篇文章或多或少包含着各种主题信息，我们更期望得到文章对于每个可能的文本类别的概率值（置信度），可以简单理解成属于对应类别的可信度。所以此时用到了soft的概念，Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=Softmax(z_%7Bi%7D)=%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\" alt=\"[公式]\"></p>\n<p>其中 <img src=\"https://www.zhihu.com/equation?tex=z_%7Bi%7D\" alt=\"[公式]\"> 为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。</p>\n<ul>\n<li><strong>引入指数形式的优点</strong></li>\n</ul>\n<p>指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，这种函数曲线能够将输出的数值拉开距离。在深度学习中通常使用反向传播求解梯度进而使用梯度下降进行参数更新的过程，而指数函数在求导的时候比较方便。比如 <img src=\"https://www.zhihu.com/equation?tex=(e%5E%7Bx%7D)%5E%7B%27%7D+=e%5E%7Bx%7D\" alt=\"[公式]\"> 。</p>\n<ul>\n<li><strong>引入指数形式的缺点</strong></li>\n</ul>\n<p>指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当 <img src=\"https://www.zhihu.com/equation?tex=z_%7Bi%7D\" alt=\"[公式]\"> 值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。</p>\n<p>当然针对数值溢出有其对应的优化方法，将每一个输出值减去输出值中最大的值。</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=D+=+max(z)+\" alt=\"[公式]\"></p>\n<p><img src=\"https://www.zhihu.com/equation?tex=softmax(z_%7Bi%7D)=%5Cfrac%7Be%5E%7Bz_%7Bi%7D+-+D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D-D%7D%7D%7D\" alt=\"[公式]\"></p>\n<p>当使用Softmax函数作为输出节点的激活函数的时候，一般使用交叉熵作为损失函数。由于Softmax函数的数值计算过程中，很容易因为输出节点的输出值比较大而发生数值溢出的现象，在计算交叉熵的时候也可能会出现数值溢出的问题。为了数值计算的稳定性，TensorFlow提供了一个统一的接口，将Softmax与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，使用TensorFlow深度学习框架的时候，一般推荐使用这个统一的接口，避免分开使用Softmax函数与交叉熵损失函数。</p>\n<pre class=\"line-numbers language-python3\" data-language=\"python3\"><code class=\"language-python3\">import tensorflow as tf\n\nprint(tf.__version__) # 2.0.0\ntf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits &#x3D; False)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>其中y_true代表了One-hot编码后的真实标签，y_pred表示网络的实际预测值：</p>\n<ul>\n<li>当from_logits设置为True时，y_pred表示未经Softmax函数的输出值；</li>\n<li>当from_logits设置为False时，y_pred表示为经过Softmax函数后的输出值；</li>\n</ul>\n<p>为了在计算Softmax函数时候数值的稳定，一般将from_logits设置为True，此时tf.keras.losses.categorical_crossentropy将在内部进行Softmax的计算，所以在不需要在输出节点上添加Softmax激活函数。</p>\n<pre class=\"line-numbers language-python3\" data-language=\"python3\"><code class=\"language-python3\">import tensorflow as tf\n\nprint(tf.__version__)\nz &#x3D; tf.random.normal([2, 10]) # 构造2个样本的10类别输出的输出值\ny &#x3D; tf.constant([1, 3]) # 两个样本的真是样本标签是1和3\ny_true &#x3D; tf.one_hot(y, depth &#x3D; 10) # 构造onehot编码\n\n# 输出层未经过Softmax激活函数,因此讲from_logits设置为True\nloss1 &#x3D; tf.keras.losses.categorical_crossentropy(y_true, z, from_logits &#x3D; True)\nloss1 &#x3D; tf.reduce_mean(loss1)\nprint(loss1) # tf.Tensor(2.6680193, shape&#x3D;(), dtype&#x3D;float32)\n\ny_pred &#x3D; tf.nn.softmax(z)\n# 输出层经过Softmax激活函数,因此讲from_logits设置为True\nloss2 &#x3D; tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits &#x3D; False)\nloss2 &#x3D; tf.reduce_mean(loss2)\nprint(loss2) # tf.Tensor(2.668019, shape&#x3D;(), dtype&#x3D;float32)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p><a href=\"https://zhuanlan.zhihu.com/p/104318223\">深度学习中常用激活函数总结</a></p>\n<h3 id=\"2-Softmax函数求导\"><a href=\"#2-Softmax函数求导\" class=\"headerlink\" title=\"2. Softmax函数求导\"></a>2. Softmax函数求导</h3><p>将Softmax函数表达式 <img src=\"https://www.zhihu.com/equation?tex=Softmax(z_%7Bi%7D)=%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\" alt=\"[公式]\"> 表示为 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"[公式]\"></p>\n<p>对于Softmax函数的梯度推导依然使用的是导数的基本运算，并不复杂。最关键的是要对<img src=\"https://www.zhihu.com/equation?tex=j+=+i\" alt=\"[公式]\"> 以及 <img src=\"https://www.zhihu.com/equation?tex=j+%5Cne+i\" alt=\"[公式]\"> 两种情况分别讨论。偏导数的最终表达式如下：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+y_%7Bi%7D%7D%7B%5Cpartial+z_%7Bj%7D%7D+=+%5Cbegin%7Bcases%7D+p_%7Bi%7D(1+-+p_%7Bj%7D)+&+j+=+i+%5C%5C+-p_%7Bj%7D%5Ccdot+p_%7Bi%7D+&+j%5Cne+i+%5C%5C+%5Cend%7Bcases%7D\" alt=\"[公式]\"> </p>\n<h3 id=\"3-交叉熵损失函数\"><a href=\"#3-交叉熵损失函数\" class=\"headerlink\" title=\"3. 交叉熵损失函数\"></a>3. 交叉熵损失函数</h3><p>假设此时第i个输出节点为正确类别对应的输出节点，则 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"[公式]\"> 是正确类别对应输出节点的概率值。添加 <img src=\"https://www.zhihu.com/equation?tex=log\" alt=\"[公式]\"> 运算不影响函数的单调性，首先为 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"[公式]\"> 添加 <img src=\"https://www.zhihu.com/equation?tex=log\" alt=\"[公式]\"> 运算：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=log%5C+p_%7Bi%7D+=+log+%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\" alt=\"[公式]\"> </p>\n<p>由于此时的 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"[公式]\"> 是正确类别对应的输出节点的概率，当然希望此时的 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"[公式]\">越大越好（当然最大不能超过1）。通常情况下使用梯度下降法来迭代求解，因此只需要为 <img src=\"https://www.zhihu.com/equation?tex=log%5C+p_%7Bi%7D\" alt=\"[公式]\"> 加上一个负号变成损失函数，现在变成希望损失函数越小越好：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=loss_%7Bi%7D++=+-log%5C+p_%7Bi%7D+=+-+log%5C+%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\" alt=\"[公式]\"> </p>\n<p>对上面的式子进一步处理：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=loss_%7Bi%7D++=+-+log%5C+%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D+=+-(z_%7Bi%7D+-+log+%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D)\" alt=\"[公式]\"> </p>\n<p>这样就将上面的Softmax一步一步转换成了Softmax的损失函数。</p>\n<p>但是通常我们说起交叉熵往往是下面的式子：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L+=+-+%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7By_%7Bc%7D%5C+log(p_%7Bc%7D)%7D\" alt=\"[公式]\"> </p>\n<p>为了方便将第一个 <img src=\"https://www.zhihu.com/equation?tex=loss_%7Bi%7D\" alt=\"[公式]\"> 命名为式子1，将通常的交叉熵损失函 <img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"[公式]\"> 数命名为式子2。其实式子1和式子2本质上是一样的。对于式子1来说，只针对正确类别的对应的输出节点，将这个位置的Softmax值最大化，而式子2则是直接衡量真实分布和实际输出的分布之间的距离。</p>\n<p>对于分类任务来说，真实的样本标签通常表示为one-hot的形式。比如对于三分类来说，真实类别的索引位置为1，也就是属于第二个类别，那么使用one-hot编码表示为[0, 1, 0]，也就是仅正确类别位置为1，其余位置都为0。而式子2中的 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bc%7D\" alt=\"[公式]\"> 就是真实样本的标签值，将[0, 1, 0]代入式子2中即 ：<img src=\"https://www.zhihu.com/equation?tex=L+=+-+%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7By_%7Bc%7D%5C+log(p_%7Bc%7D)%7D+=+-0+%5Ctimes+log(p_%7B0%7D)+-+1%5Ctimes+log(p_%7B1%7D)+-+0%5Ctimes+log(p_%7B2%7D)\" alt=\"[公式]\"> </p>\n<p>最终的结果为 <img src=\"https://www.zhihu.com/equation?tex=L+=+-1%5Ctimes+log(p_%7B1%7D)\" alt=\"[公式]\"> ，式子1只是对正确类别位置计算损失值 <img src=\"https://www.zhihu.com/equation?tex=loss_%7B1%7D++=+-+log%5C+%5Cfrac%7Be%5E%7Bz_%7B1%7D%7D%7D%7B%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D+=+-(z_%7B1%7D+-+log+%5Csum_%7Bc+=+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D)+=+-+log(p_%7B1%7D)+=+L\" alt=\"[公式]\"> </p>\n","feature":true,"text":"1.SoftmaxSoftmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。 har...","link":"","photos":[],"count_time":{"symbolsCount":"2.7k","symbolsTime":"2 mins."},"categories":[{"name":"Code","slug":"Code","count":3,"path":"api/categories/Code.json"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","count":1,"path":"api/tags/Machine-Learning.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-Softmax\"><span class=\"toc-text\">1.Softmax</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-Softmax%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC\"><span class=\"toc-text\">2. Softmax函数求导</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">3. 交叉熵损失函数</span></a></li></ol>","author":{"name":"skyrakerdu","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"while(!(succeed=try()))","socials":{"github":"https://github.com/skyrakerdu/skyrakerdu.github.io","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/skyraker-62","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/img/bilibili.svg","link":"https://live.bilibili.com/40064423"}}}},"mapped":true,"prev_post":{},"next_post":{"title":"hexo部署成功浏览器看不到新文章","uid":"8e6f1129290e74e0854044fb98335f60","slug":"hexo部署成功浏览器看不到新文章","date":"2022-01-09T04:20:26.000Z","updated":"2022-01-09T04:30:47.820Z","comments":true,"path":"api/articles/hexo部署成功浏览器看不到新文章.json","keywords":null,"cover":[],"text":"生成静态资源文件，本地执行预览命令，进入预览页面发现一切正常，然后执行部署，也没有报错，只是出现一堆警告而已，最后修改是已经提交到 git remote 仓库的。 INFO Deploy done: git 也说明了部署操作成功了。那么为什么会出现这个问题呢？ 打开控制台一看 N...","link":"","photos":[],"count_time":{"symbolsCount":601,"symbolsTime":"1 mins."},"categories":[{"name":"blogissue","slug":"blogissue","count":1,"path":"api/categories/blogissue.json"}],"tags":[{"name":"hexo","slug":"hexo","count":1,"path":"api/tags/hexo.json"}],"author":{"name":"skyrakerdu","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"while(!(succeed=try()))","socials":{"github":"https://github.com/skyrakerdu/skyrakerdu.github.io","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/skyraker-62","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/img/bilibili.svg","link":"https://live.bilibili.com/40064423"}}}},"feature":true}}